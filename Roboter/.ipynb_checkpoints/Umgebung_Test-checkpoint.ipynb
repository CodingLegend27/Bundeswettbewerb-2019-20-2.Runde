{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "# Definition einiger Farben\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "\n",
    "# Breite und HÃ¶he jedes Grids festlegen\n",
    "WIDTH = 20\n",
    "HEIGHT = 20\n",
    "\n",
    "# Rand zwischen jeder Zelle festlegen\n",
    "MARGIN = 5\n",
    "\n",
    "# Set row 1, cell 5 to one. (Remember rows and\n",
    "# column numbers start at zero.)\n",
    "grid[1][5] = 1\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set the HEIGHT and WIDTH of the screen\n",
    "WINDOW_SIZE = [255, 255]\n",
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    " \n",
    "# Set title of screen\n",
    "pygame.display.set_caption(\"Array Backed Grid\")\n",
    " \n",
    "# Loop until the user clicks the close button.\n",
    "done = False\n",
    " \n",
    "# Used to manage how fast the screen updates\n",
    "clock = pygame.time.Clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Main Program Loop -----------\n",
    "while not done:\n",
    "    for event in pygame.event.get():  # User did something\n",
    "        if event.type == pygame.QUIT:  # If user clicked close\n",
    "            done = True  # Flag that we are done so we exit this loop\n",
    "        elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            # User clicks the mouse. Get the position\n",
    "            pos = pygame.mouse.get_pos()\n",
    "            # Change the x/y screen coordinates to grid coordinates\n",
    "            column = pos[0] // (WIDTH + MARGIN)\n",
    "            row = pos[1] // (HEIGHT + MARGIN)\n",
    "            # Set that location to one\n",
    "            grid[row][column] = 1\n",
    "            print(\"Click \", pos, \"Grid coordinates: \", row, column)\n",
    " \n",
    "    # Set the screen background\n",
    "    screen.fill(BLACK)\n",
    " \n",
    "    # Draw the grid\n",
    "    for row in range(10):\n",
    "        for column in range(10):\n",
    "            color = WHITE\n",
    "            if grid[row][column] == 1:\n",
    "                color = GREEN\n",
    "            pygame.draw.rect(screen,\n",
    "                             color,\n",
    "                             [(MARGIN + WIDTH) * column + MARGIN,\n",
    "                              (MARGIN + HEIGHT) * row + MARGIN,\n",
    "                              WIDTH,\n",
    "                              HEIGHT])\n",
    " \n",
    "    # Limit to 60 frames per second\n",
    "    clock.tick(60)\n",
    " \n",
    "    # Go ahead and update the screen with what we've drawn.\n",
    "    pygame.display.flip()\n",
    " \n",
    "# Be IDLE friendly. If you forget this line, the program will 'hang'\n",
    "# on exit.\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib import style\n",
    "import time\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "SIZE = 10\n",
    "\n",
    "HM_EPISODES = 25000\n",
    "MOVE_PENALTY = 1\n",
    "ENEMY_PENALTY = 300\n",
    "FOOD_REWARD = 25\n",
    "epsilon = 0.9\n",
    "EPS_DECAY = 0.9998  # Every episode will be epsilon*EPS_DECAY\n",
    "SHOW_EVERY = 3000  # how often to play through env visually.\n",
    "\n",
    "start_q_table = None # None or Filename\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "PLAYER_N = 1  # player key in dict\n",
    "FOOD_N = 2  # food key in dict\n",
    "ENEMY_N = 3  # enemy key in dict\n",
    "\n",
    "# the dict!\n",
    "d = {1: (255, 175, 0),\n",
    "     2: (0, 255, 0),\n",
    "     3: (0, 0, 255)}\n",
    "\n",
    "\n",
    "class Blob:\n",
    "    def __init__(self):\n",
    "        self.x = np.random.randint(0, SIZE)\n",
    "        self.y = np.random.randint(0, SIZE)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.x}, {self.y}\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 4 total movement options. (0,1,2,3)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > SIZE-1:\n",
    "            self.x = SIZE-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > SIZE-1:\n",
    "            self.y = SIZE-1\n",
    "\n",
    "\n",
    "if start_q_table is None:\n",
    "    # initialize the q-table#\n",
    "    q_table = {}\n",
    "    for i in range(-SIZE+1, SIZE):\n",
    "        for ii in range(-SIZE+1, SIZE):\n",
    "            for iii in range(-SIZE+1, SIZE):\n",
    "                    for iiii in range(-SIZE+1, SIZE):\n",
    "                        q_table[((i, ii), (iii, iiii))] = [np.random.uniform(-5, 0) for i in range(4)]\n",
    "\n",
    "else:\n",
    "    with open(start_q_table, \"rb\") as f:\n",
    "        q_table = pickle.load(f)\n",
    "\n",
    "\n",
    "# can look up from Q-table with: print(q_table[((-9, -2), (3, 9))]) for example\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(HM_EPISODES):\n",
    "    player = Blob()\n",
    "    food = Blob()\n",
    "    enemy = Blob()\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"on #{episode}, epsilon is {epsilon}\")\n",
    "        print(f\"{SHOW_EVERY} ep mean: {np.mean(episode_rewards[-SHOW_EVERY:])}\")\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "\n",
    "    episode_reward = 0\n",
    "    for i in range(200):\n",
    "        obs = (player-food, player-enemy)\n",
    "        #print(obs)\n",
    "        if np.random.random() > epsilon:\n",
    "            # GET THE ACTION\n",
    "            action = np.argmax(q_table[obs])\n",
    "        else:\n",
    "            action = np.random.randint(0, 4)\n",
    "        # Take the action!\n",
    "        player.action(action)\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #enemy.move()\n",
    "        #food.move()\n",
    "        ##############\n",
    "\n",
    "        if player.x == enemy.x and player.y == enemy.y:\n",
    "            reward = -ENEMY_PENALTY\n",
    "        elif player.x == food.x and player.y == food.y:\n",
    "            reward = FOOD_REWARD\n",
    "        else:\n",
    "            reward = -MOVE_PENALTY\n",
    "        ## NOW WE KNOW THE REWARD, LET'S CALC YO\n",
    "        # first we need to obs immediately after the move.\n",
    "        new_obs = (player-food, player-enemy)\n",
    "        max_future_q = np.max(q_table[new_obs])\n",
    "        current_q = q_table[obs][action]\n",
    "\n",
    "        if reward == FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "        else:\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "        q_table[obs][action] = new_q\n",
    "\n",
    "        if show:\n",
    "            env = np.zeros((SIZE, SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "            env[food.x][food.y] = d[FOOD_N]  # sets the food location tile to green color\n",
    "            env[player.x][player.y] = d[PLAYER_N]  # sets the player tile to blue\n",
    "            env[enemy.x][enemy.y] = d[ENEMY_N]  # sets the enemy location to red\n",
    "            img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "            img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "            cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "            if reward == FOOD_REWARD or reward == -ENEMY_PENALTY:  # crummy code to hang at the end if we reach abrupt end for good reasons or not.\n",
    "                if cv2.waitKey(500) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        episode_reward += reward\n",
    "        if reward == FOOD_REWARD or reward == -ENEMY_PENALTY:\n",
    "            break\n",
    "\n",
    "    #print(episode_reward)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon *= EPS_DECAY\n",
    "\n",
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,))/SHOW_EVERY, mode='valid')\n",
    "\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.ylabel(f\"Reward {SHOW_EVERY}ma\")\n",
    "plt.xlabel(\"episode #\")\n",
    "plt.show()\n",
    "\n",
    "with open(f\"qtable-{int(time.time())}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "## Agent fÃ¼r Tutorial Bsp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# fÃ¼r was ist das TensorBoard\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# andere Variante\n",
    "#import tensorflow_addons as tfa\n",
    "#LazyAdam = tfa.optimizers.LazyAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paar Variablen\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 100\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "SHOW_PREVIEW = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob Klasse\n",
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "\n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=0)\n",
    "\n",
    "        elif choice == 6:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choice == 7:\n",
    "            self.move(x=0, y=-1)\n",
    "\n",
    "        elif choice == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob Umgebung\n",
    "\n",
    "class BlobEnv:\n",
    "    SIZE = 10\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 300\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    ACTION_SPACE_SIZE = 9\n",
    "    PLAYER_N = 1  # player key in dict\n",
    "    FOOD_N = 2  # food key in dict\n",
    "    ENEMY_N = 3  # enemy key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE)\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #enemy.move()\n",
    "        #food.move()\n",
    "        ##############\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "\n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlobEnv()\n",
    "\n",
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.compat.v1.set_random_seed(1)\n",
    "\n",
    "# Memory fraction, used mostly when trai8ning multiple agents\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "tf.compat.v1.keras.backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.compat.v1.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        # Ziel-Framework\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "        self.histories = []\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))  # OBSERVATION_SPACE_VALUES = (10, 10, 3) a 10x10 RGB image.\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Flatten()) # convert 3D Feature map zu 1D Feature Map\n",
    "        model.add(Dense(64))\n",
    "        \n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activation='linear')) # ACTION_SPACE_SIZE = how many choices (9)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        history = self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        self.histories.append(history)\n",
    "        \n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    \n",
    "                      \n",
    "                      \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0305 23:30:18.682854  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0305 23:30:18.687856  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0305 23:30:18.694857  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0305 23:30:18.714856  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0305 23:30:18.738854  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0305 23:30:18.750856  2948 deprecation.py:506] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0305 23:30:18.983856  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0305 23:30:19.247856  2948 deprecation_wrapper.py:119] From C:\\Users\\bauer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "100%|###########################################################################################################| 100/100 [11:56<00:00,  7.17s/episodes]\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 256)         7168      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 614,281\n",
      "Trainable params: 614,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(agent.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(agent.histories[-1].history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(agent.histories[-1].history['acc'])\n",
    "plt.plot(agent.histories[-1].history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(agent.histories[-1].history['loss'])\n",
    "plt.plot(agent.histories[-1].history['acc'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
