# -*- coding: utf-8 -*-
"""Stromrallye.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgSLEzkmCKLHnABqEM_s5byH_l4_iL6l

# Bundeswettbewerb Informatik
## Aufgabe 1: Stromrallye
Links zu den Beispieleingaben:

[Beispiel 1](https://bwinf.de/fileadmin/user_upload/stromrallye0.txt)
[Beispiel 2](https://bwinf.de/fileadmin/user_upload/stromrallye1.txt)
[Beispiel 3](https://bwinf.de/fileadmin/user_upload/stromrallye2.txt)
[Beispiel 4](https://bwinf.de/fileadmin/user_upload/stromrallye3.txt)
[Beispiel 5](https://bwinf.de/fileadmin/user_upload/stromrallye4.txt)
[Beispiel 5](https://bwinf.de/fileadmin/user_upload/stromrallye5.txt)

# Erstellung der Simulationsumgebung
## Steuerung
Die Simulationsumgebung wird mithilfe der gegebenen Daten (Eingabe des Benutzers) initiiert.
Die Umgebung besteht aus einem Roboter mit gegebener Bordbatterie sowie weiteren Ersatzladungen, die auf dem Spielfeld verteilt sind und deren Ladung bekannt ist.
> Vorerst sollte der Roboter nicht am Rand des Spielfelds weiter nach außen gehen können, und dabei auf der gegenüberliegenden Seite herauskommen.
Dies wird als optionale Erweiterung später betrachtet.

Zu beachten ist hierbei, dass der Roboter seine Bordbatterie mit der Ersatzbatterie austauschen muss, sobald er sich auf deren Feld befindet.

> Beispiele hierfür sind den BwInf-Websiten zu entnehmen.

### Agent
Ein außenstehender Benutzer (lernender Agent) wird in der Lage sein, über eine Methode den Roboter in der Simulationsumgebung bewegen zu können.


#### Informationsaustausch zum Agent
Daneben wird in der Simulationsumgebung eine Belohnung (Reward) für jeden Schritt vergeben, den der Agent (lernender Benutzer) tätigt.
Diese Rewards können je nach ausgeführter Handlung des Agents auch negativ ausfallen (bestrafend).

---

Neben diesen Rewards muss der lernende Agent ebenfalls wissen, wie der aktuelle 'State' (aktueller Zustand) der Simulationsumgebung ist.
> Konkret: Wo sich der Roboter und die Ersatzladungen befinden und wie hoch die jeweiligen Ladungen sind.

## Implementierung der Simulationsumgebung als eigene Klasse in Python
### Definition der Rewards
* Schritt des Roboters 
    * Reward = +1

* Austausch der Bordbatterie mit einer Ersatzbatterie 
    * Reward = +1

* Versuch gegen die Wand zu laufen
    * Reward = -1

* Alle Batterien leer
    * Reward = +100
    * Spiel wird neu gestartet

* Bordbatterie des Roboters leer
    * Reward = -100
    * Spiel wird neu gestartet

### Definition der Steuerung
verfügbare Aktion um den Robotor in der Umgebung zu steuern:
> eindeutige Definition als Zahlenwert von 0 bis 3

* nach oben: 0
* nach unten: 1
* nach links: 2
* nach rechts: 3
"""

import numpy as np
import time
import sys
if sys.version_info.major == 2:
    import Tkinter as tk
else:
    import tkinter as tk


UNIT = 40

class Environment(tk.Tk, object):
    def __init__(self, eingabe: str):
        """ erstellt eine Umgebung mit der gegebenen Eingabe """
        super(Environment, self).__init__()
        
        # Kovertiert die Eingabe
        eingabe = self.convert_eingabe(eingabe)
        self.size = eingabe.pop(0)
        
        # Die Positon des Roboters oder der Batterien werden in einem Tuple der Form (x, y, ladung) gespeichert
        self.roboter = list(eingabe.pop(0))
        self.anzahl_batterien = eingabe.pop(0)
        # die restlichen Batterien werden hinzugefügt
        self.batterien = []
        for n in range(self.anzahl_batterien):
            self.batterien.append(eingabe.pop(0))


        self.action_space = [0, 1, 2, 3]
        self.n_actions = len(self.action_space)
        self.title('Stromrallye')
        self.geometry('{0}x{1}'.format(self.size * UNIT, self.size * UNIT))
        self._build_stromrallye()


        
    def convert_eingabe(self, eingabe: str):
        """ Kovertierung der gegebenen Eingabe im gegebenen Format, siehe oben (oder auf der BwInf-Website) """
        new_eingabe = []
        eingabe = eingabe.split()
        for element in eingabe:
            if len(element) == 1:
                new_eingabe.append(int(element))
            else:
                element = list(element)
                new_element = (
                    int(element[0]),
                    int(element[2]),
                    int(element[4])
                )
                new_eingabe.append(new_element)                
        return new_eingabe
    
    def _build_stromrallye(self):
        # Canvas objekt wird erstellt
        # weißer Hintergrund mit gegebener, quadratischer Größe des Spielbretts
        self.canvas = tk.Canvas(
            self, bg='white', 
            height=self.size * UNIT,
            width=self.size * UNIT)

        # Gitter erstellen
        for spalte in range(0, self.size * UNIT, UNIT):
            x0, y0, x1, y1 = spalte, 0, spalte, self.size * UNIT
            self.canvas.create_line(x0, y0, x1, y1)
        for reihe in range(0, self.size * UNIT, UNIT):
            x0, y0, x1, y1 = 0, reihe, self.size * UNIT, reihe
            self.canvas.create_line(x0, y0, x1, y1)

        
        # IWAS origin ?!
        origin = np.array([20, 20])

        # Batterien zeichnen
        

        # IWAS ANDERES hell?!
        hell1_center = origin + np.array([UNIT * 2, UNIT])
        self.hell = self.canvas.create_rectangle(
            hell1_center[0] - 15, hell1_center[1] - 15,
            hell1_center[0] + 15, hell1_center[1] + 15,
            fill='black')
        # hell
        hell2_center = origin + np.array([UNIT, UNIT*2])
        self.hell2  = self.canvas.create_rectangle(
            hell1_center[0] - 15, hell1_center[1] - 15,
            hell1_center[0] + 15, hell1_center[1] + 15,
            fill='black')

        # create oval
        oval_center = origin + UNIT * 2
        self.oval = self.canvas.create_oval(
            oval_center[0] - 15, oval_center[1] - 15,
            oval_center[0] + 15, oval_center[1] + 15,
            fill='yellow')
        
        # create red rect
        self.rect = self.canvas.create_rectangle(
            origin[0] - 15, origin[1] - 15,
            origin[0] + 15, origin[1] + 15,
            fill='red')
            
        
        # Canvas pack
        self.canvas.pack()


    def reset(self):
        self.update()
        time.sleep(0.5)
        self.canvas.delete(self.rect)
        origin = np.array([20, 20])


    def move(self, action: int):
        """ bewegt den Roboter in der Umgebung
            nach oben: 0
            nach unten: 1
            nach links: 2
            nach rechts: 3
        """

        # TODO:
        # Roboter an Wand und kann sich nicht mehr weiter bewegen +
        # negativer Reward bei Laufen gegen die Wand
        if action == 0:
            # y-Koordinate -1
            self.roboter[1] -= 1
        elif action == 1:
            # y-Koordinate +1
            self.roboter[1] += 1 
        elif action == 2:
            # x-Koordinate -1
            self.roboter[0] -= 1
        elif action == 3:
            # x-Koordinate +1
            self.roboter[0] += 1

if __name__ == "__main__":
    
    eingabe = """
    5
    3,5,9
    3
    5,1,3
    1,2,2
    5,4,3
    """

    # erste Zeile: Größe des Spielbretts (quadratisch)
    # zweite Zeile: Koordinaten des Robotors und die Ladung seiner Batterie
    # dritte Zeile: Anzahl der restlichen Batterien, die auf dem Spielfeld verteilt sind
    # ab der vierten Zeile: Koordinaten einer Batterie, zusammen mit ihrer Ladung
    # Schreibweise: x-Koordinate, y-Koordinate, Ladung

    eingabe = """
    5
    3,5,9
    3
    5,1,3
    1,2,2
    5,4,3
    """
    env = Environment(eingabe)
    env.mainloop()

# eingabe = """
# 5
# 3,5,9
# 3
# 5,1,3
# 1,2,2
# 5,4,3
# """
# env = Environment(eingabe)

# env.move(0)
# env.move(2)
# env.roboter

# import numpy as np
# import gym

# from keras.models import Sequential
# from keras.layers import Dense, Activation, Flatten
# from keras.optimizers import Adam

# from rl.agents.dqn import DQNAgent
# from rl.policy import EpsGreedyQPolicy
# from rl.memory import SequentialMemory

# ENV_NAME = 'CartPole-v0'

# # Get the environment and extract the number of actions available in the Cartpole problem
# env = gym.make(ENV_NAME)
# np.random.seed(123)
# env.seed(123)
# nb_actions = env.action_space.n
# print(env.action_space)

# model = Sequential()
# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
# model.add(Dense(16))
# model.add(Activation('relu'))
# model.add(Dense(nb_actions))
# model.add(Activation('linear'))
# print(model.summary())

# policy = EpsGreedyQPolicy()
# memory = SequentialMemory(limit=50000, window_length=1)
# dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
#     target_model_update=1e-2,
#     policy=policy
#     )
# dqn.compile(Adam(lr=1e-3), metrics=['mae'])

# # Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. 
# dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)